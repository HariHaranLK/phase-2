### **README.md**

---

# ğŸ¤ **Speech Sentiment Analysis Using Deep Learning** ğŸš€

## ğŸŒŸ **Project Overview**
This project performs **speech sentiment analysis** using multiple deep learning models to classify audio data into emotional categories such as **positive, negative, and neutral**. The system processes raw speech signals by converting them into **spectrograms**, which are then fed into various **pre-trained and custom-built deep learning models** for feature extraction and classification.  
<br>  

<div align="center">
  <img src="https://github.com/user-attachments/assets/8a346e7c-83ae-40b7-93f0-d62cedba33cb" width="600" height="300" />
</div>

  

---

## âš™ï¸ **Technologies Used**
- **ğŸ› ï¸ Programming Language:** Python  
- **ğŸ“š Libraries & Frameworks:**  
  - ğŸ **TensorFlow, Keras** â€“ Model training and evaluation  
  - ğŸ“Š **NumPy, Pandas** â€“ Data manipulation  
  - ğŸµ **Librosa** â€“ Audio processing  
  - ğŸ“ˆ **Matplotlib** â€“ Visualization  
- **ğŸ” Models Used:**  
  - ğŸŒ€ **ResNet-50**  
  - ğŸš€ **InceptionV3**  
  - ğŸ”¥ **VGG16**  
  - âš¡ **DenseNet-121**  
  - âœ¨ **Xception**  
  - ğŸ¯ **CNN**  
  - ğŸš¦ **EfficientNetB7**  

---

## ğŸ”¥ **Workflow**
1. ğŸ™ï¸ **Data Preprocessing:**  
   - Load raw audio files.  
   - Convert them into **spectrograms** for visual representation.  
   - Normalize and resize the spectrograms for model compatibility.  

2. âš™ï¸ **Model Training:**  
   - Train multiple models on the spectrogram dataset.  
   - Use **Adam optimizer** with cross-entropy loss.  
   - Apply techniques like **batch normalization and dropout** for stability.  

3. ğŸ“Š **Evaluation:**  
   - Validate the models using metrics like **accuracy, loss, precision, and recall**.  
   - Compare performance across different architectures.  
<br> 

---

## ğŸš€ **Installation**
1. Clone the repository:
```
git clone <your-repo-link>
```
2. Install dependencies:
```
pip install -r requirements.txt
```
3. Run the project:
```
python main.py
```

---

## âœ… **Usage**
- **ğŸ”Š Input:** Raw audio files  
- **ğŸ¯ Output:** Predicted sentiment labels (**positive, neutral, negative**)  
- **ğŸ“Š Visualization:** Accuracy and loss graphs  

---

## ğŸ“ˆ **Results**
- **âœ¨ Xception and InceptionV3** demonstrated the highest accuracy, around **97%**, with low loss values.  
- **âš¡ CNN and DenseNet-121** provided efficient performance with moderate computational costs.  
- **ğŸ”¥ VGG16 and ResNet-50** showed slight overfitting but maintained high accuracy.  
<br>  
![image](https://github.com/user-attachments/assets/c22ed786-8b63-4de2-ace1-08057b143422)  

---

## ğŸš€ **Future Enhancements**
- ğŸ› ï¸ **Real-time Speech Sentiment Analysis:** Implementing live audio input for real-time classification.  
- ğŸ”— **Integration with Speech-to-Text:** Combining sentiment and content analysis for richer insights.  
- ğŸ“Š **Fine-Tuning Models:** Using larger and more diverse datasets to improve generalization.  

---

## ğŸ‘¥ **Contributors**
- ğŸ§‘â€ğŸ’» **HARI HARAN L K**  
- ğŸ§‘â€ğŸ“ **KANNAN G**  
- ğŸ§‘â€ğŸ“ **SANJAY A**  
<br>  
![Teamwork](https://media.giphy.com/media/l3q2z2e5u7duuh1H2/giphy.gif)  

---

## ğŸ“œ **License**
This project is licensed under the **MIT License**.  
<br>  
![License](https://media.giphy.com/media/l0HlQ7LRalNBSuLzq/giphy.gif)  

---

âœ… Let me know if you need any more tweaks or additions! ğŸš€
